{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport datetime\nimport itertools\nfrom collections import defaultdict\n\nimport math\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import shapiro\nimport scipy.stats as stats\n\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV, ElasticNet\nfrom xgboost import XGBRegressor\nimport catboost as cb\nimport lightgbm as lgb\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit, KFold, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,OrdinalEncoder\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error, make_scorer\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders import TargetEncoder\nfrom category_encoders.one_hot import OneHotEncoder\nfrom sklearn.compose import make_column_selector as selector","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:36.959007Z","iopub.execute_input":"2023-11-16T08:20:36.959443Z","iopub.status.idle":"2023-11-16T08:20:46.510039Z","shell.execute_reply.started":"2023-11-16T08:20:36.959411Z","shell.execute_reply":"2023-11-16T08:20:46.509003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configs","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:46.512366Z","iopub.execute_input":"2023-11-16T08:20:46.512739Z","iopub.status.idle":"2023-11-16T08:20:46.517745Z","shell.execute_reply.started":"2023-11-16T08:20:46.512703Z","shell.execute_reply":"2023-11-16T08:20:46.516830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"dir_path = '/kaggle/input/store-sales-time-series-forecasting/'\n\noil_df = pd.read_csv(dir_path + 'oil.csv')\nholidays_df = pd.read_csv(dir_path + 'holidays_events.csv')\nstores_df = pd.read_csv(dir_path + 'stores.csv')\ntrain_df = pd.read_csv(dir_path + 'train.csv')\ntest_df = pd.read_csv(dir_path + 'test.csv')\ntrnsctns_df = pd.read_csv(dir_path + 'transactions.csv')\nsubmissions_df = pd.read_csv(dir_path + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:46.519178Z","iopub.execute_input":"2023-11-16T08:20:46.519456Z","iopub.status.idle":"2023-11-16T08:20:50.358455Z","shell.execute_reply.started":"2023-11-16T08:20:46.519430Z","shell.execute_reply":"2023-11-16T08:20:50.357371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.359678Z","iopub.execute_input":"2023-11-16T08:20:50.359983Z","iopub.status.idle":"2023-11-16T08:20:50.380551Z","shell.execute_reply.started":"2023-11-16T08:20:50.359955Z","shell.execute_reply":"2023-11-16T08:20:50.379650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holidays_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.383454Z","iopub.execute_input":"2023-11-16T08:20:50.383995Z","iopub.status.idle":"2023-11-16T08:20:50.412523Z","shell.execute_reply.started":"2023-11-16T08:20:50.383967Z","shell.execute_reply":"2023-11-16T08:20:50.411614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stores_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.413680Z","iopub.execute_input":"2023-11-16T08:20:50.413993Z","iopub.status.idle":"2023-11-16T08:20:50.427244Z","shell.execute_reply.started":"2023-11-16T08:20:50.413965Z","shell.execute_reply":"2023-11-16T08:20:50.426328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.428469Z","iopub.execute_input":"2023-11-16T08:20:50.428767Z","iopub.status.idle":"2023-11-16T08:20:50.445769Z","shell.execute_reply.started":"2023-11-16T08:20:50.428740Z","shell.execute_reply":"2023-11-16T08:20:50.444975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.446813Z","iopub.execute_input":"2023-11-16T08:20:50.447103Z","iopub.status.idle":"2023-11-16T08:20:50.461147Z","shell.execute_reply.started":"2023-11-16T08:20:50.447078Z","shell.execute_reply":"2023-11-16T08:20:50.460194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trnsctns_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.462390Z","iopub.execute_input":"2023-11-16T08:20:50.462668Z","iopub.status.idle":"2023-11-16T08:20:50.476318Z","shell.execute_reply.started":"2023-11-16T08:20:50.462643Z","shell.execute_reply":"2023-11-16T08:20:50.475310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.477653Z","iopub.execute_input":"2023-11-16T08:20:50.478212Z","iopub.status.idle":"2023-11-16T08:20:50.492844Z","shell.execute_reply.started":"2023-11-16T08:20:50.478182Z","shell.execute_reply":"2023-11-16T08:20:50.491957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-processing","metadata":{}},{"cell_type":"markdown","source":"#### Change 'date' column type","metadata":{}},{"cell_type":"code","source":"oil_df['date'] = pd.to_datetime(oil_df['date'], format = \"%Y-%m-%d\")\nholidays_df['date'] = pd.to_datetime(holidays_df['date'], format = \"%Y-%m-%d\")\ntrnsctns_df['date'] = pd.to_datetime(trnsctns_df['date'], format = \"%Y-%m-%d\")\ntrain_df['date'] = pd.to_datetime(train_df['date'], format = \"%Y-%m-%d\")\ntest_df['date'] = pd.to_datetime(test_df['date'], format = \"%Y-%m-%d\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.493949Z","iopub.execute_input":"2023-11-16T08:20:50.494533Z","iopub.status.idle":"2023-11-16T08:20:50.953605Z","shell.execute_reply.started":"2023-11-16T08:20:50.494500Z","shell.execute_reply":"2023-11-16T08:20:50.952810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fill missing values in datasets","metadata":{}},{"cell_type":"code","source":"train_data_strt_dt = train_df['date'].min()\ntrain_data_end_dt = train_df['date'].max()\ntrain_dt_rnge = pd.date_range(start=train_data_strt_dt, end=train_data_end_dt)\ntrain_missing_dts = train_dt_rnge.difference(train_df['date'])\n\ntest_data_strt_dt = test_df['date'].min()\ntest_data_end_dt = test_df['date'].max()\ntest_dt_rnge = pd.date_range(start=test_data_strt_dt, end=test_data_end_dt)\ntest_missing_dts = test_dt_rnge.difference(test_df['date'])\n\nprint(f\"Missing dates in training set: {train_missing_dts}\")\nprint(f\"Missing dates in test set: {test_missing_dts}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:50.954682Z","iopub.execute_input":"2023-11-16T08:20:50.954974Z","iopub.status.idle":"2023-11-16T08:20:51.053123Z","shell.execute_reply.started":"2023-11-16T08:20:50.954935Z","shell.execute_reply":"2023-11-16T08:20:51.051871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a Multi-index variable","metadata":{}},{"cell_type":"code","source":"multi_index = pd.MultiIndex.from_product([pd.date_range(train_data_strt_dt, train_data_end_dt),\n                                          train_df.store_nbr.unique(),\n                                          train_df.family.unique()],\n                                         names=['date','store_nbr','family'],)\ntrain_df = train_df.set_index(['date','store_nbr','family']).reindex(multi_index).reset_index()\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:51.055043Z","iopub.execute_input":"2023-11-16T08:20:51.055418Z","iopub.status.idle":"2023-11-16T08:20:52.544922Z","shell.execute_reply.started":"2023-11-16T08:20:51.055382Z","shell.execute_reply":"2023-11-16T08:20:52.543853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fill missing values with 0s","metadata":{}},{"cell_type":"code","source":"train_df[['sales','onpromotion']] = train_df[['sales','onpromotion']].fillna(0.)\n\n# Apply linear interpolation to the \"id\" column to estimate the missing values based on the linear relationship between adjacent data points. \ntrain_df.id = train_df.id.interpolate(method=\"linear\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:52.549629Z","iopub.execute_input":"2023-11-16T08:20:52.549925Z","iopub.status.idle":"2023-11-16T08:20:52.721124Z","shell.execute_reply.started":"2023-11-16T08:20:52.549899Z","shell.execute_reply":"2023-11-16T08:20:52.720372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add an additional column in both training and test sets to separate those two","metadata":{}},{"cell_type":"code","source":"train_df['test'] = 0\ntest_df['test'] = 1","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:52.722206Z","iopub.execute_input":"2023-11-16T08:20:52.722501Z","iopub.status.idle":"2023-11-16T08:20:52.728367Z","shell.execute_reply.started":"2023-11-16T08:20:52.722473Z","shell.execute_reply":"2023-11-16T08:20:52.727643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Oil Data - Re-index by adding missing dates","metadata":{}},{"cell_type":"code","source":"# Create a date range from the start of training data to the end of test data\ndate_range = pd.date_range(train_data_strt_dt, test_data_end_dt)\n\n# Create a DataFrame with the date range\ndate_df = pd.DataFrame({'date': date_range})\n\n# Merge the date_df with oil_df using an outer join\noil_df = oil_df.merge(date_df, on = 'date', how = 'outer')\n\n# Sort the DataFrame by date and reset the index\noil_df = oil_df.sort_values('date', ignore_index=True)\n\n# fill missing values using linear interpolation\noil_df['dcoilwtico'] = oil_df['dcoilwtico'].interpolate(method=\"linear\", limit_direction=\"both\")\n\noil_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:52.729393Z","iopub.execute_input":"2023-11-16T08:20:52.729636Z","iopub.status.idle":"2023-11-16T08:20:52.754421Z","shell.execute_reply.started":"2023-11-16T08:20:52.729613Z","shell.execute_reply":"2023-11-16T08:20:52.753466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transactions Data - Fill in the missing values for transactions using interpolation, except for days with zero sales","metadata":{}},{"cell_type":"code","source":"# Calculate the number of unique store numbers\nnum_store = train_df['store_nbr'].nunique()\n\n# Calculate the number of days in the training period\ntrain_len = (train_data_end_dt - train_data_strt_dt).days + 1\n\n# Calculate the number of records where sales are zero\nnum_zero_sales = (train_df.groupby([\"date\", \"store_nbr\"])['sales'].sum() == 0).sum()\n\n# Calculate the total number of expected records\ntotal_rec = num_store * train_len\n\n# Calculate the current number of records\ncurr_rec = len(trnsctns_df.index)\n\n# Calculate the number of missing records\nmissing_rec = total_rec - curr_rec - num_zero_sales\n\n# Total sales for each store\nstore_sales = train_df.groupby([\"date\", \"store_nbr\"]).sales.sum().reset_index()\n\n# Re-index transaction data\ntrnsctns_df = trnsctns_df.merge(store_sales, on=[\"date\", \"store_nbr\"],how=\"outer\").sort_values([\"date\", \"store_nbr\"],ignore_index=True)\n\n\n# Fill missing values with 0s for days with zero sales\ntrnsctns_df.loc[trnsctns_df.sales.eq(0), \"transactions\"] = 0\n\n# Drop the \"sales\" column\ntrnsctns_df = trnsctns_df.drop(columns=[\"sales\"])\n\n# Fill remaining missing values using linear interpolation within each \"store_nbr\" group\ntrnsctns_df[\"transactions\"] = trnsctns_df.groupby(\"store_nbr\")[\"transactions\"].transform(lambda x: x.interpolate(method=\"linear\", limit_direction=\"both\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:52.755466Z","iopub.execute_input":"2023-11-16T08:20:52.755700Z","iopub.status.idle":"2023-11-16T08:20:53.201613Z","shell.execute_reply.started":"2023-11-16T08:20:52.755678Z","shell.execute_reply":"2023-11-16T08:20:53.200825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Holidays Data - Remove transferred holidays, and separate work days from the main DataFrame for further analysis","metadata":{}},{"cell_type":"code","source":"# Define a function to process holiday descriptions\ndef process_holiday(s):\n    # Check if \"futbol\" is in the description; if so, return \"futbol\"\n    if \"futbol\" in s:\n        return \"futbol\"\n    \n    # Create a list of words to remove based on cities and states\n    to_remove = list(set(stores_df['city'].str.lower()) | set(stores_df['state'].str.lower()))\n    \n    # Iterate through the list of words and remove them from the description\n    for w in to_remove:\n        s = s.replace(w, \"\")\n    \n    # Return the processed description\n    return s","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:53.202713Z","iopub.execute_input":"2023-11-16T08:20:53.203019Z","iopub.status.idle":"2023-11-16T08:20:53.208713Z","shell.execute_reply.started":"2023-11-16T08:20:53.202992Z","shell.execute_reply":"2023-11-16T08:20:53.207837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean and process the 'description' column in the holidays_data DataFrame\nholidays_df['description'] = holidays_df.apply(lambda x: x['description'].lower().replace(x['locale_name'].lower(), \"\"), axis=1).apply(process_holiday).replace(r\"[+-]\\d+|\\b(de|del|traslado|recupero|puente|-)\\b\", \"\", regex=True).replace(r\"\\s+|-\", \" \", regex=True).str.strip()\n\n# Remove transferred holidays from the DataFrame\nholidays_df = holidays_df[holidays_df['transferred'].eq(False)]\n\n# Extract and process work days\nwork_days = holidays_df[holidays_df['type'].eq(\"Work Day\")]\nwork_days = work_days[[\"date\", \"type\"]].rename(columns={\"type\": \"work_day\"}).reset_index(drop=True)\n\n# Convert the 'work_day' column to binary values (1 for work days, 0 for others)\nwork_days['work_day'] = work_days['work_day'].notna().astype(int)\n\n# Remove work days from the main holidays_data DataFrame\nholidays_df = holidays_df[holidays_df['type']!=\"Work Day\"].reset_index(drop=True)\n\nholidays_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:53.210029Z","iopub.execute_input":"2023-11-16T08:20:53.210866Z","iopub.status.idle":"2023-11-16T08:20:53.342935Z","shell.execute_reply.started":"2023-11-16T08:20:53.210831Z","shell.execute_reply":"2023-11-16T08:20:53.342126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Local holidays at city level with dummy variables for descriptions","metadata":{}},{"cell_type":"code","source":"# Filter local holidays at the city level\nlocal_holidays = holidays_df[holidays_df['locale'].eq(\"Local\")]\n\n# Select relevant columns, rename 'locale_name' to 'city', and reset the index\nlocal_holidays = local_holidays[[\"date\", \"locale_name\", \"description\"]].rename(columns={\"locale_name\": \"city\"}).reset_index(drop=True)\n\n# Remove duplicated rows\nlocal_holidays = local_holidays[~local_holidays.duplicated()]\n\n# Create dummy variables for 'description' and prefix them with \"loc\"\nlocal_holidays = pd.get_dummies(local_holidays, columns=[\"description\"], prefix=\"loc\")\n\n# Display the resulting DataFrame\nlocal_holidays.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:53.344177Z","iopub.execute_input":"2023-11-16T08:20:53.344617Z","iopub.status.idle":"2023-11-16T08:20:53.362265Z","shell.execute_reply.started":"2023-11-16T08:20:53.344583Z","shell.execute_reply":"2023-11-16T08:20:53.361346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regional holidays are filtered and processed to include a binary column indicating whether the description contains \"provincializacion.\"","metadata":{}},{"cell_type":"code","source":"# Filter regional holidays\nregional_holidays = holidays_df[holidays_df['locale'].eq(\"Regional\")]\n\n# Select relevant columns and rename 'locale_name' to 'state' and 'description' to 'provincializacion'\nregional_holidays = regional_holidays[[\"date\", \"locale_name\", \"description\"]].rename(columns={\"locale_name\": \"state\", \"description\": \"provincializacion\"}).reset_index(drop=True)\n\n# Create a binary column 'provincializacion' based on the presence of the word \"provincializacion\" in descriptions\nregional_holidays['provincializacion'] = regional_holidays['provincializacion'].eq(\"provincializacion\").astype(int)\n\n# Return the resulting DataFrame\nregional_holidays.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:53.363443Z","iopub.execute_input":"2023-11-16T08:20:53.363814Z","iopub.status.idle":"2023-11-16T08:20:53.391743Z","shell.execute_reply.started":"2023-11-16T08:20:53.363764Z","shell.execute_reply":"2023-11-16T08:20:53.390805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"National holidays are filtered, processed, and organized for further analysis.","metadata":{}},{"cell_type":"code","source":"# Filter national holidays\nnational_holidays = holidays_df[holidays_df['locale'].eq(\"National\")]\n\n# Select relevant columns and reset the index\nnational_holidays = national_holidays[[\"date\", \"description\"]].reset_index(drop=True)\n\n# Remove duplicated rows\nnational_holidays = national_holidays[~national_holidays.duplicated()]\n\n# Create dummy variables for 'description' and prefix them with \"nat\"\nnational_holidays = pd.get_dummies(national_holidays, columns=[\"description\"], prefix=\"nat\")\n\n# Group national holidays that fall on the same date and sum the binary values\nnational_holidays = national_holidays.groupby(\"date\").sum().reset_index()\n\n# Rename columns for visualization purposes\nnational_holidays = national_holidays.rename(columns={\"nat_primer grito independencia\": \"nat_primer grito\"})\n\n# Return the resulting DataFrame\nnational_holidays.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:53.393007Z","iopub.execute_input":"2023-11-16T08:20:53.393280Z","iopub.status.idle":"2023-11-16T08:20:53.423312Z","shell.execute_reply.started":"2023-11-16T08:20:53.393255Z","shell.execute_reply":"2023-11-16T08:20:53.422466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pivot and reshape data into time series format for Sales, Transactions, and Promotions.","metadata":{}},{"cell_type":"code","source":"# Pivot the train_data DataFrame to create a time series of sales data\nsales_ts = pd.pivot_table(train_df, values=\"sales\", index=\"date\", columns=[\"store_nbr\", \"family\"])\n\n# Pivot the transactions_data DataFrame to create a time series of transaction data\ntr_ts = pd.pivot_table(trnsctns_df, values=\"transactions\", index=\"date\", columns=\"store_nbr\")\n\n# Pivot the train_data DataFrame to create a time series of promotion data\npromo_ts = pd.pivot_table(train_df, values=\"onpromotion\", index=\"date\", columns=[\"store_nbr\", \"family\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:53.424515Z","iopub.execute_input":"2023-11-16T08:20:53.424896Z","iopub.status.idle":"2023-11-16T08:20:56.230757Z","shell.execute_reply.started":"2023-11-16T08:20:53.424851Z","shell.execute_reply":"2023-11-16T08:20:56.229729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_ts.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:56.232255Z","iopub.execute_input":"2023-11-16T08:20:56.232629Z","iopub.status.idle":"2023-11-16T08:20:57.322029Z","shell.execute_reply.started":"2023-11-16T08:20:56.232593Z","shell.execute_reply":"2023-11-16T08:20:57.321139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create master dataset for time series analysis","metadata":{}},{"cell_type":"code","source":"# scale target series\nscaler = MinMaxScaler()\nsales_ts_scaled = sales_ts.copy()\nsales_ts_scaled[sales_ts_scaled.columns] = scaler.fit_transform(sales_ts_scaled)\n\n# convert back to long form and add the holiday columns\nholiday_sales_merged = sales_ts_scaled.melt(\n    value_name=\"sales\", ignore_index=False,).reset_index().\\\n    merge(stores_df, on=\"store_nbr\", how=\"left\").\\\n    merge(work_days, on=\"date\", how=\"left\").\\\n    merge(local_holidays, on=[\"date\", \"city\"], how=\"left\").\\\n    merge(regional_holidays, on=[\"date\", \"state\"], how=\"left\").\\\n    merge(national_holidays, on=\"date\", how=\"left\").\\\n    fillna(0)\n\n# include dummy variable for dates without any holidays\nholiday_list = [col for col in holiday_sales_merged if col.startswith((\"loc_\", \"nat_\", \"provincializacion\"))]\nholiday_sales_merged[\"no_holiday\"] = holiday_sales_merged[holiday_list].sum(axis=1).eq(0).astype(int)\n\nholiday_sales_merged.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:20:57.323224Z","iopub.execute_input":"2023-11-16T08:20:57.323505Z","iopub.status.idle":"2023-11-16T08:21:18.820853Z","shell.execute_reply.started":"2023-11-16T08:20:57.323479Z","shell.execute_reply":"2023-11-16T08:21:18.819831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge data","metadata":{}},{"cell_type":"code","source":"# Define a list of selected national holidays with larger impacts on sales\nselected_holidays = [\"nat_terremoto\", \"nat_navidad\", \"nat_dia la madre\", \"nat_dia trabajo\",\n                     \"nat_primer dia ano\", \"nat_futbol\", \"nat_dia difuntos\"]\n\n# Select only the columns related to the selected national holidays\nkeep_national_holidays = national_holidays[[\"date\", *selected_holidays]]\n\n\n# Concatenate the train and test data along the rows\ndata = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n\n# Merge the 'stores_data' DataFrame on the 'store_nbr' column\ndata = data.merge(stores_df, on=[\"store_nbr\"])\n\n# Merge the 'oil_data' DataFrame on the 'date' column, with a left join\ndata = data.merge(oil_df, on=[\"date\"], how=\"left\")\n\n# Merge the 'transactions_data' DataFrame on the 'date' and 'store_nbr' columns, with a left join\ndata = data.merge(trnsctns_df, on=[\"date\", 'store_nbr'], how=\"left\")\n\n# Merge the 'work_days' DataFrame on the 'date' column, with a left join\ndata = data.merge(work_days, on=\"date\", how=\"left\")\n\n# Merge the 'keep_national_holidays' DataFrame on the 'date' column, with a left join\ndata = data.merge(keep_national_holidays, on=[\"date\"], how=\"left\")\n\n# Sort the resulting dataset by 'date', 'store_nbr', and 'family' columns\ndata = data.sort_values([\"date\", \"store_nbr\", \"family\"], ignore_index=True)\n\n\n# The last section of code fills missing values in the 'work_day' and selected holiday columns with 0:\ndata[[\"work_day\", *selected_holidays]] = data[[\"work_day\", *selected_holidays]].fillna(0)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:21:18.822422Z","iopub.execute_input":"2023-11-16T08:21:18.823135Z","iopub.status.idle":"2023-11-16T08:21:23.714342Z","shell.execute_reply.started":"2023-11-16T08:21:18.823094Z","shell.execute_reply":"2023-11-16T08:21:23.713325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Add new columns that provide temporal information about the data","metadata":{}},{"cell_type":"code","source":"# Select the date, days of the week, hours, month (not used in calculations)\ndata['day_of_week'] = data.date.dt.dayofweek\ndata['day_of_year'] = data.date.dt.dayofyear\ndata['day_of_month'] = data.date.dt.day\ndata['year'] = data.date.dt.year\ndata['month'] = data.date.dt.month\n\n# Seasons: 0-winter; 1-spring; 2-summer; 3-fall\ndata[\"season\"] = np.where(data.date.dt.month.isin([12, 1, 2]), 0, 1)\ndata[\"season\"] = np.where(data.date.dt.month.isin([3, 4, 5]), 1, data[\"season\"])\ndata[\"season\"] = np.where(data.date.dt.month.isin([6, 7, 8]), 2, data[\"season\"])\ndata[\"season\"] = np.where(data.date.dt.month.isin([9, 10, 11]), 3, data[\"season\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:21:23.715319Z","iopub.execute_input":"2023-11-16T08:21:23.715592Z","iopub.status.idle":"2023-11-16T08:21:24.477246Z","shell.execute_reply.started":"2023-11-16T08:21:23.715566Z","shell.execute_reply":"2023-11-16T08:21:24.476157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the EDA, it was found that the sales across stores are distributed unevenly. If there are any memory limitations, we will consider only a part of the stores","metadata":{}},{"cell_type":"code","source":"#Let's keep data for 1-20 stores, due to the lack of memory\n#data = data.loc[data['store_nbr'].isin(list(range(1, 19)))]","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:21:24.478611Z","iopub.execute_input":"2023-11-16T08:21:24.478945Z","iopub.status.idle":"2023-11-16T08:21:24.482994Z","shell.execute_reply.started":"2023-11-16T08:21:24.478901Z","shell.execute_reply":"2023-11-16T08:21:24.482172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a copy of the original data\ndata_analyses = data.copy()\n\n# Specify the target variable\ntarget = 'sales'\n\n# Separate the data into training and testing sets based on the 'test' column\ntrain = data_analyses.loc[data_analyses['test'] == 0]\ntest = data_analyses.loc[data_analyses['test'] == 1]\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:21:24.484190Z","iopub.execute_input":"2023-11-16T08:21:24.484457Z","iopub.status.idle":"2023-11-16T08:21:25.529147Z","shell.execute_reply.started":"2023-11-16T08:21:24.484433Z","shell.execute_reply":"2023-11-16T08:21:25.528210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rolling Summary Stats Features\n#A rolling mean is simply the mean of a certain number of previous periods in a time series.\nfor i in [16,17,18,19,20,21,22,46,76,106,365, 730]:\n    data_analyses[\"sales_roll_mean_\"+str(i)]=data_analyses.groupby([\"store_nbr\", \"family\"])['sales'].rolling(i).mean().shift(1).values","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:21:25.530316Z","iopub.execute_input":"2023-11-16T08:21:25.530599Z","iopub.status.idle":"2023-11-16T08:21:49.063589Z","shell.execute_reply.started":"2023-11-16T08:21:25.530573Z","shell.execute_reply":"2023-11-16T08:21:49.062588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lag/ Shifted Features","metadata":{}},{"cell_type":"code","source":"data_analyses.sort_values(by=['store_nbr', 'family', 'date'], axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:21:49.064711Z","iopub.execute_input":"2023-11-16T08:21:49.065028Z","iopub.status.idle":"2023-11-16T08:21:51.603506Z","shell.execute_reply.started":"2023-11-16T08:21:49.065001Z","shell.execute_reply":"2023-11-16T08:21:51.602756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lag_features(dataframe, lags, groups = [\"store_nbr\", \"family\"], target = \"sales\", prefix = ''):\n    dataframe = dataframe.copy()\n    for lag in lags:\n        dataframe[prefix + str(lag)] = dataframe.groupby(groups)[target].transform(lambda x: x.shift(lag))\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:21:51.604589Z","iopub.execute_input":"2023-11-16T08:21:51.604852Z","iopub.status.idle":"2023-11-16T08:21:51.610235Z","shell.execute_reply.started":"2023-11-16T08:21:51.604828Z","shell.execute_reply":"2023-11-16T08:21:51.609265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's create lags\ndata_analyses = lag_features(data_analyses, \n                             lags = [16,17,18,19,20,21,22,46,76,106,365, 730],\n                             groups = [\"store_nbr\", \"family\"], target = 'sales', \n                             prefix = 'sales_lag_')\ndata_analyses.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:21:51.611351Z","iopub.execute_input":"2023-11-16T08:21:51.611628Z","iopub.status.idle":"2023-11-16T08:22:01.904433Z","shell.execute_reply.started":"2023-11-16T08:21:51.611603Z","shell.execute_reply":"2023-11-16T08:22:01.903512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the most correlated features\ndef drop_cor(dataframe, name, index):\n    ind = dataframe[dataframe.columns[dataframe.columns.str.contains(name)].tolist()+[\n        \"sales\"]].corr().sales.sort_values(ascending = False).index[1:index]\n    ind = dataframe.drop(ind, axis = 1).columns[dataframe.drop(ind, axis = 1).columns.str.contains(name)]\n    dataframe.drop(ind, axis = 1, inplace = True)\n\ndrop_cor(data_analyses, \"sales_lag\", 6)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:22:01.905394Z","iopub.execute_input":"2023-11-16T08:22:01.905635Z","iopub.status.idle":"2023-11-16T08:22:04.474213Z","shell.execute_reply.started":"2023-11-16T08:22:01.905611Z","shell.execute_reply":"2023-11-16T08:22:04.473431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train/ Validation Split","metadata":{}},{"cell_type":"code","source":"# Dataframe must be sorted by date because of Time Series Split \ndata_analyses = data_analyses.sort_values(\"date\").reset_index(drop=True)\n\n# Let's bring all the columns into a single form to avoid further errors\ndata_analyses.columns = [column.replace(\" \", \"_\") for column in data_analyses.columns]\n\n# Define the columns that will be further used in the analysis\nfeatures = [col for col in data_analyses.columns if col not in ['date', 'id', \"sales\", 'transactions',\n                                                                'day_of_week','day_of_year','day_of_month',\n                                                                'year', 'month', 'season','test']]\n\n# Fill NA for all columns with 0 or appropriate empty value based on data type\nfor col in data_analyses.columns:\n    fill_value = 0 if data_analyses[col].dtype in [int, float] else pd.NA\n    data_analyses[col] = data_analyses[col].fillna(fill_value)\n\n# Make the data readable\ndata_analyses['store_nbr'] = data_analyses['store_nbr'].apply(lambda x: (f\"store_nbr_{x}\"))\ndata_analyses['cluster'] = data_analyses['cluster'].apply(lambda x: (f\"cluster_{x}\"))\ndata_analyses['type'] = data_analyses['type'].apply(lambda x: (f\"type_{x}\"))\ndata_analyses['city'] = data_analyses['city'].apply(lambda x: (f\"city_{x.lower()}\"))\ndata_analyses['state'] = data_analyses['state'].apply(lambda x: (f\"state_{x.lower()}\"))\n\ndata_analyses.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:22:04.475456Z","iopub.execute_input":"2023-11-16T08:22:04.476221Z","iopub.status.idle":"2023-11-16T08:22:15.010682Z","shell.execute_reply.started":"2023-11-16T08:22:04.476181Z","shell.execute_reply":"2023-11-16T08:22:15.009657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Divide the data into training and validation and use them at the stage of identifying important features\ntrain_f = data_analyses.copy()\nval_f = data_analyses.copy()\n\ntrain_f = train_f.loc[(train_f[\"date\"] < \"2017-01-01\"), :]\nval_f = val_f.loc[(val_f[\"date\"] >= \"2017-01-01\") & (val_f[\"date\"] < \"2017-08-16\"), :]\n\nY_train = train_f['sales']\nX_train = train_f[features]\n\nY_val = val_f['sales']\nX_val = val_f[features]\n\n# Define object columns\nobject_cols = X_train.loc[:,X_train.dtypes==object].columns\nobject_cols = list(object_cols)\ncols_for_le = object_cols \ncols_for_le = [list(X_train.columns).index(col) for col in cols_for_le]\n\n# Transform categorical features \nt = [('MeanTargetEncoder', TargetEncoder(), cols_for_le)]\ncol_transform = ColumnTransformer(transformers=t)\ncol_transform.set_output(transform=\"pandas\")\nX_trans_tr =col_transform.fit_transform(X_train,Y_train)\nX_val_tr =col_transform.fit_transform(X_val,Y_val)\n\nY_train.shape, X_trans_tr.shape, Y_val.shape, X_val_tr.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:22:15.011955Z","iopub.execute_input":"2023-11-16T08:22:15.012262Z","iopub.status.idle":"2023-11-16T08:22:45.635820Z","shell.execute_reply.started":"2023-11-16T08:22:15.012236Z","shell.execute_reply":"2023-11-16T08:22:45.634840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importance","metadata":{}},{"cell_type":"code","source":"for c in object_cols:\n    X_train[c] = X_train[c].astype('category')\n    X_val[c] = X_val[c].astype('category')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:22:45.637225Z","iopub.execute_input":"2023-11-16T08:22:45.637637Z","iopub.status.idle":"2023-11-16T08:22:47.496342Z","shell.execute_reply.started":"2023-11-16T08:22:45.637598Z","shell.execute_reply":"2023-11-16T08:22:47.494651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SMAPE: Symmetric mean absolute percentage error\ndef smape(preds, target):\n    smape_val=1/len(target) * np.sum(2 * np.abs(preds-target) / (np.abs(target) + np.abs(preds))*100)\n    return smape_val","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:22:47.498228Z","iopub.execute_input":"2023-11-16T08:22:47.498635Z","iopub.status.idle":"2023-11-16T08:22:47.504926Z","shell.execute_reply.started":"2023-11-16T08:22:47.498596Z","shell.execute_reply":"2023-11-16T08:22:47.503924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_model = lgb.LGBMRegressor(random_state=384).fit(X_train, Y_train,\n                                                      eval_metric= lambda y_true,\n                                                      y_pred: [mean_squared_error(y_true, y_pred)],\n                                                      categorical_feature = object_cols)\n\nprint(\"TRAIN SMAPE:\", smape(Y_train, first_model.predict(X_train)))\nprint(\"VALID SMAPE:\", smape(Y_val, first_model.predict(X_val)))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:22:47.506309Z","iopub.execute_input":"2023-11-16T08:22:47.506710Z","iopub.status.idle":"2023-11-16T08:23:09.285091Z","shell.execute_reply.started":"2023-11-16T08:22:47.506673Z","shell.execute_reply":"2023-11-16T08:23:09.284291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_lgb_importances(model, plot=False, num=120):\n    # SKLEARN API\n    gain = model.booster_.feature_importance(importance_type='gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name_,\n                             'split': model.booster_.feature_importance(importance_type='split'),\n                             'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n        return feat_imp\n\nfeature_imp_df = plot_lgb_importances(first_model, num=200)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:23:09.286365Z","iopub.execute_input":"2023-11-16T08:23:09.286916Z","iopub.status.idle":"2023-11-16T08:23:09.299464Z","shell.execute_reply.started":"2023-11-16T08:23:09.286885Z","shell.execute_reply":"2023-11-16T08:23:09.298487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_lgb_importances(first_model, plot=True, num=10)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:23:09.306212Z","iopub.execute_input":"2023-11-16T08:23:09.306476Z","iopub.status.idle":"2023-11-16T08:23:10.127575Z","shell.execute_reply.started":"2023-11-16T08:23:09.306452Z","shell.execute_reply":"2023-11-16T08:23:10.126644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature importance\ncols = feature_imp_df[feature_imp_df.gain > 0.015].feature.tolist()\nprint(\"Independent Variables:\", len(cols))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:23:10.128942Z","iopub.execute_input":"2023-11-16T08:23:10.129279Z","iopub.status.idle":"2023-11-16T08:23:10.136731Z","shell.execute_reply.started":"2023-11-16T08:23:10.129244Z","shell.execute_reply":"2023-11-16T08:23:10.135713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[cols]","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:23:10.137994Z","iopub.execute_input":"2023-11-16T08:23:10.138274Z","iopub.status.idle":"2023-11-16T08:23:10.302273Z","shell.execute_reply.started":"2023-11-16T08:23:10.138248Z","shell.execute_reply":"2023-11-16T08:23:10.301270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"code","source":"train = data_analyses.loc[data_analyses['test'] == 0]\ntest = data_analyses.loc[data_analyses['test'] == 1]\n\nX = train[cols]\ny = train[target]","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:23:10.303544Z","iopub.execute_input":"2023-11-16T08:23:10.303871Z","iopub.status.idle":"2023-11-16T08:23:11.566391Z","shell.execute_reply.started":"2023-11-16T08:23:10.303841Z","shell.execute_reply":"2023-11-16T08:23:11.565580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_folds = 5\ntscv = TimeSeriesSplit(n_splits=num_folds)\n\n#creating dictionaries to record results\nmse_scores = defaultdict(list)\nrmse_scores = defaultdict(list)\nr2_scores = defaultdict(list)\nmae_scores = defaultdict(list)\nmape_scores = defaultdict(list)\nsmape_scores = defaultdict(list)\nmodels = defaultdict(list)\n\n# Metrics used to evaluate models\ndef metrics_regression(y_true, y_pred):\n    # MSE\n    mse = mean_squared_error(y_true, y_pred) #!\n    \n    # RMSE (Root Mean Square Error)\n    rmse = math.sqrt(mse)\n    \n    # R^2\n    r2 = r2_score(y_true, y_pred)\n    \n    # MAE(mean absolute error)\n    mae = mean_absolute_error(y_true, y_pred) #!\n    \n    # MAPE(mean absolute percentage error)\n    mape = mean_absolute_percentage_error(y_true, y_pred) #!\n    \n    #SMAPE (symmetric mean absolute percentage error)\n    smape = 1/len(y_true) * np.sum(2 * np.abs(y_pred-y_true) / (np.abs(y_true) + np.abs(y_pred))*100)\n    \n    return mse,rmse,r2,mae,mape,smape","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:23:11.567506Z","iopub.execute_input":"2023-11-16T08:23:11.567812Z","iopub.status.idle":"2023-11-16T08:23:11.575945Z","shell.execute_reply.started":"2023-11-16T08:23:11.567771Z","shell.execute_reply":"2023-11-16T08:23:11.574979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGBM Regressor","metadata":{}},{"cell_type":"code","source":"%%time\nfor fold_idx, (train_index, test_index) in enumerate(tscv.split(train)):\n    print(f\"Fold {fold_idx + 1}\")\n\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_lgbm = lgb.LGBMRegressor(random_state=42,n_estimators=10000, colsample_bytree=0.5,\n                                   device_type='gpu', verbose=3)\n    \n    object_cols = list(X_train.loc[:,X_train.dtypes==object].columns)\n    \n    for c in object_cols:\n        X_train[c] = X_train[c].astype('category')\n        X_test[c] = X_test[c].astype('category')\n\n    model_lgbm.fit(X_train, y_train,\n                   categorical_feature = object_cols)\n\n    y_pred_LGBM = model_lgbm.predict(X_test)\n\n    models['lgbm'].append(model_lgbm)\n    \n    mse_LGBM,rmse_LGBM,r2_LGBM,mae_LGBM,mape_LGBM,smape_LGBM = metrics_regression(y_test, y_pred_LGBM)\n\n    mse_scores['lgbm'].append(mse_LGBM)\n    rmse_scores['lgbm'].append(rmse_LGBM)\n    r2_scores['lgbm'].append(r2_LGBM)\n    mae_scores['lgbm'].append(mae_LGBM)\n    mape_scores['lgbm'].append(mape_LGBM)\n    smape_scores['lgbm'].append(smape_LGBM)\n    \n    #print(f\"\\t Score for LGBM: {mse_scores,rmse_scores, r2_scores,mae_scores,mape_scores,smape_scores}\")\n    print('*'*60)\n\nprint(f\"\\t\\t Mean MSE \\n\\t LGBM: {np.mean(mse_scores['lgbm'])}\")\nprint(f\"\\t\\t Mean RMSE \\n\\t LGBM: {np.mean(rmse_scores['lgbm'])}\")\nprint(f\"\\t\\t Mean SMAPE \\n\\t LGBM: {np.mean(smape_scores['lgbm'])}\")\nprint(f\"\\t\\t Mean R2 \\n\\t LGBM: {np.mean(r2_scores['lgbm'])}\")\nprint(f\"\\t\\t Mean MAE \\n\\t LGBM: {np.mean(mae_scores['lgbm'])}\")\nprint(f\"\\t\\t Mean MAPE \\n\\t LGBM: {np.mean(mape_scores['lgbm'])}\")\nprint('*'*60)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-16T08:23:11.577039Z","iopub.execute_input":"2023-11-16T08:23:11.577286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"result = model_lgbm.predict(test[cols])\nsample_submission = pd.DataFrame({'id':test.id,'sales':result}).set_index('id')\nsample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}