{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-12T15:39:09.188959Z","iopub.execute_input":"2023-11-12T15:39:09.189299Z","iopub.status.idle":"2023-11-12T15:39:24.845601Z","shell.execute_reply.started":"2023-11-12T15:39:09.189267Z","shell.execute_reply":"2023-11-12T15:39:24.844171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root = '/kaggle/input/store-sales-time-series-forecasting/'\n\ntrain_data = pd.read_csv(os.path.join(root, 'train.csv'), index_col=0)\ntest_data_orig = pd.read_csv(os.path.join(root, 'test.csv'), index_col=0)\noil_df = pd.read_csv(os.path.join(root, 'oil.csv'))\nsamp_subm = pd.read_csv(os.path.join(root, 'sample_submission.csv'))\nholiday_event_df = pd.read_csv(os.path.join(root, 'holidays_events.csv'))\nstores_df =  pd.read_csv(os.path.join(root, 'stores.csv'))\ntransactions_df = pd.read_csv(os.path.join(root, 'transactions.csv'))","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:24.848480Z","iopub.execute_input":"2023-11-12T15:39:24.849890Z","iopub.status.idle":"2023-11-12T15:39:28.024018Z","shell.execute_reply.started":"2023-11-12T15:39:24.849827Z","shell.execute_reply":"2023-11-12T15:39:28.022782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge\ntrain_data = train_data.merge(stores_df, on = 'store_nbr') \ntrain_data = train_data.merge(oil_df, on ='date', how = 'left')\nholiday_event_df = holiday_event_df.rename(columns = {'type': 'holiday'})\nholiday_event_df['holiday'] = holiday_event_df['holiday'].apply(lambda x: 1 if x == 'Holiday' else 0)\nholiday_event_df['holiday'].fillna(0, inplace=True)\n\ntrain_data = train_data.merge(holiday_event_df, on = 'date', how = 'left')\ntest_data = test_data_orig.merge(holiday_event_df, on = 'date', how = 'left')\n\ntrain_data['holiday'].fillna(0, inplace=True)\ntest_data['holiday'].fillna(0, inplace=True)\n\ntest_data['sales'] = 0.0\ntest_data_orig['sales'] = 0.0","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:28.025364Z","iopub.execute_input":"2023-11-12T15:39:28.025780Z","iopub.status.idle":"2023-11-12T15:39:30.687556Z","shell.execute_reply.started":"2023-11-12T15:39:28.025742Z","shell.execute_reply":"2023-11-12T15:39:30.686507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['holiday'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:30.689595Z","iopub.execute_input":"2023-11-12T15:39:30.689915Z","iopub.status.idle":"2023-11-12T15:39:30.713646Z","shell.execute_reply.started":"2023-11-12T15:39:30.689890Z","shell.execute_reply":"2023-11-12T15:39:30.712396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"req_columns = ['sales', 'onpromotion', 'holiday']\nreq_columns_agg = {'sales':'mean', 'onpromotion':'mean', 'holiday': 'max'}\n# req_columns = {'sales':'mean'}\ntrain_data = train_data.groupby(['date']).agg(req_columns_agg)\ntest_data = test_data.groupby(['date']).agg(req_columns_agg)\n\ntrain_data = train_data[req_columns]\ntest_data = test_data[req_columns]","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:30.714740Z","iopub.execute_input":"2023-11-12T15:39:30.715074Z","iopub.status.idle":"2023-11-12T15:39:31.123900Z","shell.execute_reply.started":"2023-11-12T15:39:30.715048Z","shell.execute_reply":"2023-11-12T15:39:31.122728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.125390Z","iopub.execute_input":"2023-11-12T15:39:31.125703Z","iopub.status.idle":"2023-11-12T15:39:31.145238Z","shell.execute_reply.started":"2023-11-12T15:39:31.125677Z","shell.execute_reply":"2023-11-12T15:39:31.143922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.146794Z","iopub.execute_input":"2023-11-12T15:39:31.147138Z","iopub.status.idle":"2023-11-12T15:39:31.161485Z","shell.execute_reply.started":"2023-11-12T15:39:31.147101Z","shell.execute_reply":"2023-11-12T15:39:31.160166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_sequences(data, seq_length):\n    sequences = []\n    for i in range(len(data) - seq_length):\n        seq = data[i:i+seq_length]\n        # label = data[i+seq_length]\n        sequences.append(seq)\n    # print(sequences)\n    return np.array(sequences)\n    \ndef get_ytrain(train_data, seq_length):\n    # Extract the 'sales' column from the train data\n    y_train = train_data['sales'].values.reshape(-1, 1)\n    print(len(y_train))\n    return y_train[seq_length:]\n\ndef get_xtrain_xtest(train_data, test_data, req_columns, seq_length=30):\n    # Normalize the data\n    # shift on promotion and holiday by one?\n    # test_data_scaled = None\n    # if add_cols:\n    #     scaler = MinMaxScaler()\n    #     scaler.fit_transform(train_data[add_cols])\n    #     test_data_scaled = scaler.transform(test_data[add_cols])\n    \n    scaler = MinMaxScaler()\n    train_data_scaled = scaler.fit_transform(train_data[req_columns])\n    # test_data_scaled = scaler.transform(test_data[req_columns])\n    \n    train_data_scaled = create_sequences(train_data_scaled, seq_length)\n\n    # return train_data_scaled, test_data_scaled, scaler\n    return train_data_scaled, scaler\n\ndef get_next_seq(n, new_sale_val, train_data_scaled, test_data, scaler, seq_length):\n    # create new instance in test_data\n    new_instance = test_data.values[n]\n    new_instance[0] = new_sale_val\n    new_instance = np.array([new_instance])\n    \n    # transform the data to be added in xtrain\n    print(f'New instance before transform: {new_instance}')\n    new_instance_scaled =  scaler.transform(new_instance)\n    print(f'New instance after transform: {new_instance_scaled}')\n    \n    # insert new value to x_train\n    new_sequance = train_data_scaled[-1][1:]\n    new_sequance = np.vstack([new_sequance, new_instance_scaled[0]])\n    train_data_scaled = np.concatenate([train_data_scaled, new_sequance[np.newaxis, :, :]])\n    # np.insert(train_data_scaled, len(train_data_scaled), new_instance_scaled[0], axis=0)\n    \n    # get new sequance. last seq_length items in x_train\n    new_sequance = train_data_scaled[-1]\n    new_sequance = np.array([new_sequance])    \n    # print(new_sequance)\n    \n    return train_data_scaled, new_sequance\n\ndef get_predictions(model, test_data_orig, test_data, x_train, y_train, scaler, seq_length):\n    new_sale_val = y_train[-1]\n    for i in range(len(test_data)):\n        x_train, new_sequance = get_next_seq(i, new_sale_val, x_train, test_data, scaler, seq_length)\n\n        # predict\n        sales_predicted = model.predict(new_sequance)\n        print(sales_predicted)\n        new_sale_val = sales_predicted[-1][0]\n        date = test_data.index[i]\n        print(f'Predict value for {date} : {new_sale_val}')\n        \n        # add predicted value to test_data\n        test_data.iloc[i]['sales'] = new_sale_val\n    \n    return test_data","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.163034Z","iopub.execute_input":"2023-11-12T15:39:31.163413Z","iopub.status.idle":"2023-11-12T15:39:31.176804Z","shell.execute_reply.started":"2023-11-12T15:39:31.163368Z","shell.execute_reply":"2023-11-12T15:39:31.175626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_length=30\nx_train, scaler = get_xtrain_xtest(train_data, test_data, req_columns, seq_length=seq_length)\ny_train = get_ytrain(train_data, seq_length)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.178258Z","iopub.execute_input":"2023-11-12T15:39:31.178671Z","iopub.status.idle":"2023-11-12T15:39:31.201229Z","shell.execute_reply.started":"2023-11-12T15:39:31.178631Z","shell.execute_reply":"2023-11-12T15:39:31.200110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.205558Z","iopub.execute_input":"2023-11-12T15:39:31.206488Z","iopub.status.idle":"2023-11-12T15:39:31.213304Z","shell.execute_reply.started":"2023-11-12T15:39:31.206452Z","shell.execute_reply":"2023-11-12T15:39:31.212291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.214982Z","iopub.execute_input":"2023-11-12T15:39:31.215302Z","iopub.status.idle":"2023-11-12T15:39:31.228895Z","shell.execute_reply.started":"2023-11-12T15:39:31.215277Z","shell.execute_reply":"2023-11-12T15:39:31.227797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate_model(model, x_train, y_train, fit_args, n_splits=5):\n    # Initialize lists to store evaluation metrics\n    mse_scores = []\n    mae_scores = []\n    \n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    i = 0\n    # Perform k-fold cross-validation\n    for train_index, val_index in tscv.split(x_train):\n        # Split the data into training and validation sets\n#         print(train_index)\n        X_train, X_val = x_train[train_index], x_train[val_index]\n        Y_train, Y_val = y_train[train_index], y_train[val_index]\n        \n        # Train the model\n        model.fit(X_train, Y_train, **fit_args)\n\n        # Make predictions on the validation set\n        val_predictions = model.predict(X_val)\n\n        # Evaluate the model\n        mse = mean_squared_error(Y_val, val_predictions)\n        print(f'mse of fold: {mse}')\n        mae = mean_absolute_error(Y_val, val_predictions)\n        print(f'mse of fold: {mae}')\n\n        mse_scores.append(mse)\n        mae_scores.append(mae)\n        i += 1\n    # Display the average performance metrics across folds\n    print(f'Average Mean Squared Error: {np.mean(mse_scores)}')\n    print(f'Average Mean Absolute Error: {np.mean(mae_scores)}')","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.230152Z","iopub.execute_input":"2023-11-12T15:39:31.231282Z","iopub.status.idle":"2023-11-12T15:39:31.241661Z","shell.execute_reply.started":"2023-11-12T15:39:31.231183Z","shell.execute_reply":"2023-11-12T15:39:31.240516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(seq_length, x_train.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\nfit_args = {'epochs': 500, 'batch_size': 32, 'verbose': 0}","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.243437Z","iopub.execute_input":"2023-11-12T15:39:31.243890Z","iopub.status.idle":"2023-11-12T15:39:31.538488Z","shell.execute_reply.started":"2023-11-12T15:39:31.243823Z","shell.execute_reply":"2023-11-12T15:39:31.537357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate_model(model, x_train, y_train, fit_args, n_splits=3)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:39:31.539947Z","iopub.execute_input":"2023-11-12T15:39:31.540327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.fit(X_train[:, :-1], X_train[:, -1], epochs=50, batch_size=32)\n\nmodel.fit(x_train, y_train, **fit_args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = get_predictions(model, test_data_orig, test_data, x_train, y_train, scaler, seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = test_data_orig.reset_index()\nsubmission_df = submission_df.drop(columns=['sales'])\nsubmission_df = pd.merge(submission_df, test_data, on='date', how='left')\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = submission_df[['id', 'sales']]\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_subm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/submission.csv')\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}